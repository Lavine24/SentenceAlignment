#include "alignment_models/model1.h"

#include <cassert>
#include <limits>

#include "util/math_util.h"

using std::numeric_limits;

void Model1::InitDataStructures(const vector<const ParallelCorpus*>& pcs,
                                const Vocab& total_source_vocab,
                                const Vocab& total_target_vocab) {
  source_vocab_size_ = total_source_vocab.size();
  target_vocab_size_ = total_target_vocab.size();
  t_table_ = new PackedTrie();
  t_table_->InitializeFromCorpus(pcs, total_source_vocab, total_target_vocab);
  expected_counts_ = new PackedTrie(*t_table_);
}

void Model1::InitFromFile(const string& filename, Vocab* source_vocab,
    Vocab* target_vocab) {
  t_table_ = new PackedTrie();
  t_table_->Read(filename, source_vocab, target_vocab);
  expected_counts_ = new PackedTrie(*t_table_);
  source_vocab_size_ = source_vocab->size();
  target_vocab_size_ = target_vocab->size();
}

void Model1::InitFromBinaryFile(const string& t_table_file,
                                const string& source_vocab_file,
                                const string& target_vocab_file,
                                Vocab* source_vocab,
                                Vocab* target_vocab) {
  t_table_ = new PackedTrie();
  t_table_->ReadBinary(t_table_file);
  source_vocab->Read(source_vocab_file);
  target_vocab->Read(target_vocab_file);
  expected_counts_ = new PackedTrie(*t_table_);
  source_vocab_size_ = source_vocab->size();
  target_vocab_size_ = target_vocab->size();
}

double Model1::ScorePair(const Sentence& source, const Sentence& target) const {
  double result = 0.0;
  // The uniform alignment probability.
  double alignment_prob = log(1.0 / target.size());
  for (int t = 0; t < target.size(); ++t) {
    // The probability of generating this target word, initialized with the
    // probability of it being generated by the null source word.
    double t_prob = t_table_->Prob(0, target[t]);
    for (int s = 0; s < source.size(); ++s) {
      t_prob = MathUtil::LogAdd(t_prob,
                                t_table_->Prob(source[s], target[t]));
    }
    // Add the probability of generating this word to the 
    // The alignment probability can be factored out.
    result += t_prob + alignment_prob;
  }
  return result;
}

double Model1::ViterbiScorePair(const Sentence& source, const Sentence& target)
    const {
  double result = 0.0;
  // The uniform alignment probability.
  double alignment_prob = log(1.0 / target.size());
  for (int t = 0; t < target.size(); ++t) {
    // The probability of generating this target word, initialized with the
    // probability of it being generated by the null source word.
    double max_prob = t_table_->Prob(0, target[t]);
    for (int s = 0; s < source.size(); ++s) {
      double p = t_table_->Prob(source[s], target[t]);
      if (p > max_prob) {
        max_prob = p;
      }
    }
    // Add the probability of generating this word to the 
    // The alignment probability can be factored out.
    result += max_prob + alignment_prob;
  }
  return result;
}

double Model1::ComputeCoverage(const Sentence& source, const Sentence& target,
    double log_word_cutoff, bool covered_unk, bool ignored_unk) const {
  double coverage = 0.0;
  int target_size = target.size();
  for (int t = 0; t < target.size(); ++t) {
    if (covered_unk && (target[t] > target_vocab_size_)) {
      ++coverage;
      continue;
    } else if (ignored_unk && (target[t] > target_vocab_size_)) {
      --target_size;
      continue;
    }
    /*
    if (t_table_->Prob(0, target[t]) > log_cutoff) {
      ++coverage;
      continue;
    }*/
    for (int s = 0; s < source.size(); ++s) {
      double p = t_table_->Prob(source[s], target[t]);
      if (p > log_word_cutoff) {
        ++coverage;
        break;
      }
    }
  }
  if (target_size == 0) {
    return 0.0;
  } else {
    return coverage / target_size;
  }
}

void Model1::ClearExpectedCounts() {
  expected_counts_->Clear();
}

double Model1::EStep(
    const Sentence& source, const Sentence& target, double weight) {
  // The uniform alignment probability.
  double result = 0.0;
  double alignment_prob = log(1.0 / target.size());
  int* indices = new int[source.size()];
  for (int t = 0; t < target.size(); ++t) {
    // Since entries in the T-Table are accessed multiple times, remember the
    // index of each entry for re-use.
    int null_index = t_table_->FindIndex(0, target[t]);

    // The probability of generating this target word, initialized with the
    // probability of it being generated by the null source word.
    double t_prob = t_table_->Data(null_index);
    for (int s = 0; s < source.size(); ++s) {
      indices[s] = t_table_->FindIndex(source[s], target[t]);
      t_prob = MathUtil::LogAdd(t_prob, t_table_->Data(indices[s]));
    }
    // Add the probability of generating this word to the 
    // The alignment probability can be factored out.
    result += t_prob + alignment_prob;
    // Update the expected counts using t_prob as the denominator
    // Null word generation
    MathUtil::LogPlusEQ(expected_counts_->Data(null_index),
                        t_table_->Data(null_index) - t_prob + weight);
    for (int s = 0; s < source.size(); ++s) {
      MathUtil::LogPlusEQ(expected_counts_->Data(indices[s]),
                          t_table_->Data(indices[s]) - t_prob + weight);
    }
  }
  delete[] indices;
  return result;
}

void Model1::MStep(bool variational) {
  if (!variational) {
    if (alpha_ < 1.0) {
      std::cerr << "Can't use a regular M-Step with an alpha less than 1"
          << std::endl;
      assert(0);
    }
    for (int s = 0; s < source_vocab_size_; ++s) {
      double sum = -numeric_limits<double>::max();
      int min = expected_counts_->Offset(s);
      int max = expected_counts_->Offset(s+1);
      for (int i = min; i < max; ++i) {
        MathUtil::LogPlusEQ(expected_counts_->Data(i), log(alpha_ - 1.0));
        MathUtil::LogPlusEQ(sum, expected_counts_->Data(i));
      }
      if (sum > -numeric_limits<double>::max()) {
        for (int i = min; i < max; ++i) {
          t_table_->Data(i) = expected_counts_->Data(i) - sum;
        }
      }
    }
  } else {
    // Variational Update
    assert(0); // TODO: Redo
    /*
    // Clear the denominators
    for (int i = 0; i < source_vocab_size_; ++i) {
      Model1::lookup(&expected_counts_, i, 0) = -numeric_limits<double>::max();
    }
    TTable::iterator it;
    for (it = expected_counts_.begin(); it != expected_counts_.end(); ++it) {
      if (it->first.second > 0) {
        MathUtil::LogPlusEQ(
            Model1::lookup(&expected_counts_, it->first.first, 0),
            MathUtil::LogAdd(it->second, log(alpha_)));
        it->second = exp(math_util::Digamma(exp(it->second) + alpha_));
      }
    }
    for (it = t_table_.begin(); it != t_table_.end(); ++it) {
      // c(t,s) / c(s) in the log domain
      it->second = log(expected_counts_[it->first]
          / exp(math_util::Digamma(exp(Model1::lookup(&expected_counts_,
                it->first.first, 0)))));
    }
    // Renormalization (TODO: Not sure if this is right)
    for (int i = 0; i < source_vocab_size_; ++i) {
      Model1::lookup(&expected_counts_, i, 0) = -numeric_limits<double>::max();
    }
    for (it = t_table_.begin(); it != t_table_.end(); ++it) {
      MathUtil::LogPlusEQ(
          Model1::lookup(&expected_counts_, it->first.first, 0), it->second);
    }
    for (it = t_table_.begin(); it != t_table_.end(); ++it) {
      it->second -= Model1::lookup(&expected_counts_, it->first.first, 0);
    }
    */
  }
}

void Model1::PrintTTable(const Vocab& source_vocab,
                         const Vocab& target_vocab,
                         std::ostream& out) const {
  t_table_->Print(source_vocab, target_vocab, out);
}

void Model1::WriteBinary(const string& t_table_file,
                         const string& source_vocab_file,
                         const string& target_vocab_file,
                         const Vocab& source_vocab,
                         const Vocab& target_vocab) const {
  t_table_->WriteBinary(t_table_file);
  source_vocab.Write(source_vocab_file);
  target_vocab.Write(target_vocab_file);
}

